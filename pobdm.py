# -*- coding: utf-8 -*-
"""PoBDM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cdW73QVX9Cq_r-i5kloUv2OSAOibyf8H

Upload kaggle.json to access datasets

Install pyspark
"""

!pip install pyspark

"""Install kaggle and access kaggle permissions with kaggle.json"""

!pip install kaggle
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Import necessary libraries (Some of these may be removed for final version)"""

import pandas as pd
import numpy as np
import nltk
import pyspark.pandas as ps
import random

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover
from pyspark.ml.feature import StringIndexer, CountVectorizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""Create Spark session"""

spark = SparkSession.builder.appName('Restaurant Data').getOrCreate()

"""Get restaurant data"""

!kaggle datasets download inigolopezrioboo/a-tripadvisor-dataset-for-nlp-tasks
!unzip a-tripadvisor-dataset-for-nlp-tasks.zip

"""Create Spark dataframe using London reviews"""

df = spark.read.csv('London_reviews.csv', inferSchema = True, header = True)

"""**Preparing Data**

a. Understanding Data and removing unwanted columns

b. Filtering neutral reviews

c. Assigning Positive and Negative Sentiment to Reviews based on Score (luckily this already exists in our dataset)

d. Assigning Binary Rating as Target Variable 1: Positive 0: Negative

We will use the following settings to check in on our progress at each step. However, this will add to our run time since .show is an action. This data already comes with a column 'sample' that lists if a review is positive or negative. This is useful for building an initial model.

This is what our initial RDD looks like.
"""

df.show(truncate = False, n = 5)
df.count()

"""Our first step is to drop all columns other than restaurant_name, rating_review, sample, and review_full."""

df = df[['restaurant_name', 'rating_review', 'sample', 'review_full']]
#df.show(truncate = False, n = 5)

"""Next we noticed that "rating_review" actually had string type values, so we converted them to double for future manipulation."""

df.printSchema()
df = df.withColumn('rating_review', df.rating_review.cast('double'))
df.printSchema()

"""After taking a deeper look at our data, we realized that in several rows the entries seem to be in the wrong column (something that is clearly a review being present under "restaurant_name", for instance). This could affect our model in a negative way, so we want to remove those rows.

Additionally, we want to minimize how much weight our model is based on restaurants with a very low number of reviews. There are several restaurants in the data that have an average 5.0 review, which is realistically only the case because they have such a small sample size.

We can address both of these problems by removing rows which have restaurant_names that appear more than one time in the entire dataset.
"""

uniquedf = df.groupBy('restaurant_name').agg(countDistinct('review_full')).orderBy('count(review_full)', ascending = True)
#uniquedf.show(truncate = False, n = 5)
#uniquedf.count()

"""Looking at the whole dataset we discovered that the first restaurant to have more than one unique review was in the 275th row of our new 2100 row rdd."""

print(uniquedf.collect()[273][0], ", ", uniquedf.collect()[273][1])
print(uniquedf.collect()[274][0], ", ", uniquedf.collect()[274][1])

"""We created a new rdd of unique restaurant names with greater than one unique review by dropping the first 274 rows of uniquedf. From this, we made a list called restaurants based on the first column of this rdd."""

uniquedf2 = spark.createDataFrame(uniquedf.tail(uniquedf.count()-274), uniquedf.schema)
#uniquedf2.show(n = 5)
#uniquedf2.count()

restaurants = uniquedf2.select('restaurant_name').collect()
rest_list = [row.restaurant_name for row in restaurants]
print(rest_list)

"""Now we can drop all rows which have restaurant_names outside of this new rdd."""

df = df.filter(df.restaurant_name.isin(rest_list))

"""Now that the nonsense rows have been removed, we can move on to our next data cleaning step by removing all rows with null values.

---


"""

df = df.dropna()
#df.show(truncate = False, n = 5)

"""First Checkpoint. If anything goes wrong later, we can start over with this."""

check_df1 = df

"""**Text Pre Processing **

a. Create UDF Functions for text processing: Convert to lower case, Remove Punctuations and alphanumeric words, Remove Stop words

b. Text Lemmatization

i) Convert reviews to lower case.
"""

df = check_df1.select("*", lower(col('review_full')))
df = df.drop(col('review_full'))
df = df.withColumnRenamed('lower(review_full)', 'review_full')
#df.show(truncate = False, n = 5)

"""ii) Remove punctuation and alphanumeric words

For this we create a python function to leave only the lowercase letters and a space, then convert this function into a spark UDF.
"""

def only_letters(x): #ord(' ') == 32
  stripped = (c for c in x \
              if ord(c) == 32 \
              or 96 < ord(c) < 123) #ord('a') - ord('z') = 97-122
  return ''.join(stripped)

only_letters_udf = udf(lambda x: only_letters(x))

df = df.select(col('*')
          , only_letters_udf(col('review_full')))
df = df.drop(col('review_full'))
df = df.withColumnRenamed('<lambda>(review_full)', 'review_full')
#df.show(truncate = False, n = 5)

"""Second checkpoint."""

check_df2 = df

"""iii) Remove stopwords

Before we remove the stopwords we need to tokenize the text.
"""

tokenizer = Tokenizer(inputCol = 'review_full', outputCol = 'tokens')
df = tokenizer.transform(check_df2)
df = df.drop(col('review_full'))
df = df.withColumnRenamed('tokens', 'review_full')

"""Once the reviews are tokenzied, we can use the pyspark StopWordsRemover to remove the stopwords (as one might guess)."""

remover = StopWordsRemover(inputCol = 'review_full'
  , outputCol = 'review_clean')
df = remover.transform(df).select('*')
df = df.drop(col('review_full'))
df = df.withColumnRenamed('review_clean', 'review_full')
#df.show(truncate = False, n = 5)

"""Third Checkpoint."""

check_df3 = df

"""iv) Lemmatize"""

lemmatizer = WordNetLemmatizer()
def lemm(x):
  lems = []
  for word in x:
    temp = lemmatizer.lemmatize(word)
    lems.append(temp)
  return ', '.join(lems)

lemm_udf = udf(lambda x: lemm(x))

df = check_df3.select(col('*'), lemm_udf(col('review_full')))
df = df.drop(col('review_full'))
df = df.withColumnRenamed('<lambda>(review_full)', 'review_full')
#df.show(truncate = False, n = 5)

"""To create our sentiment analysis model, we will start by splitting our dataset into training data and testing data. The model will be designed using only the training data, and will then be evaluated against the testing data."""

(train_set, test_set) = df.randomSplit([0.8, 0.2], seed = 1213)

"""Creating a logistic regression pipeline.

Tokenizer -> TF/IDF -> logistic regression model
"""

tokenizer = Tokenizer(inputCol = 'review_full', outputCol = 'words')
hashtf = HashingTF(numFeatures = 2**16, inputCol = 'words', outputCol = 'tf')
idf = IDF(inputCol = 'tf', outputCol = 'features', minDocFreq = 5)
label_stringIdx = StringIndexer(inputCol = 'rating_review', outputCol = 'label')
pipeline = Pipeline(stages = [tokenizer, hashtf, idf, label_stringIdx])

pipelineFit = pipeline.fit(train_set)

"""Fitting the pipeline along the training set and testing set."""

train_df = pipelineFit.transform(train_set)
test_df = pipelineFit.transform(test_set)
#train_df.show(n = 5)

"""Creating the predictive model based on the logistic regression on the training set."""

lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter = 100)
lrModel = lr.fit(train_df)
prediction = lrModel.transform(test_df)

"""Creating a new RDD which shows the rating_review and the predicted review based on our model."""

lrdf = lrModel.summary.predictions.select('restaurant_name', 'review_full', \
                                          'rating_review', 'prediction')
lrdf = lrdf.withColumn('rating_prediction', col('prediction') - 5)
lrdf = lrdf.drop('prediction')
lrdf = lrdf.withColumn('prediction', col('rating_prediction')*-1)
lrdf = lrdf.drop('rating_prediction')

lrdf.show(n = 20)

"""We can determine how accurate our model is by viewing the proportion of predictions that actually match our label."""

prediction = prediction.select('restaurant_name', 'review_full', \
                                          'rating_review', 'prediction')
prediction = prediction.withColumn('rating_prediction', col('prediction') - 5)
prediction = prediction.drop('prediction')
prediction = prediction.withColumn('prediction', col('rating_prediction')*-1)
prediction = prediction.drop('rating_prediction')

prediction.sample(.0001).show()

"""Here we take the number of rows where the actual review matches our predicted review and divide by the number of total rows."""

hits = prediction.filter(prediction[2] == \
                         prediction[3]).count()
total = prediction.count()

print(hits/total)

"""Creating a UDF to convert a rating_review to a word, 'Positive', 'Negative', or 'Neutral'. Then using this UDF to convert our previous testing set RDD to words."""

def sent(x):
  if x == 3:
    return 'Neutral'
  elif x > 3:
    return 'Positive'
  else:
    return 'Negative'

sent_udf = udf(lambda x: sent(x))

prediction = prediction.select(col('*'), sent_udf(col('rating_review')))
prediction = prediction.drop(col('rating_review'))
prediction = prediction.withColumnRenamed('<lambda>(rating_review)', 'off_sent')
#prediction.show(n = 5)

prediction = prediction.select(col('*'), sent_udf(col('prediction')))
prediction = prediction.drop(col('prediction'))
prediction = prediction.withColumnRenamed('<lambda>(prediction)', 'pred_sent')
#prediction.show(n = 5)

prediction.sample(.0001).show()

"""Determining how many rows have a sentiment review and predicted sentiment that match. The 'total' number of rows remains unchanged."""

sent_hits = prediction.filter(prediction.off_sent == \
                         prediction.pred_sent).count()

print(sent_hits/total)

"""Last step, word cloud based on positive/negative reviews"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import ChainMap
import pyspark.sql.functions as F

cloud_df = df.drop('restaurant_name', 'sample')

cloud_df.show(n = 5)

cloud_df = cloud_df.select(col('*'), sent_udf(col('rating_review')))
cloud_df = cloud_df.drop(col('rating_review'))
cloud_df = cloud_df.withColumnRenamed('<lambda>(rating_review)', 'sent')
cloud_df.show(n = 5)

pos_df = cloud_df.filter(cloud_df.sent == 'Positive')
neg_df = cloud_df.filter(cloud_df.sent == 'Negative')

pos_df.show(n = 5)

neg_df.show(n = 5)

wordcloud = WordCloud(background_color = 'white')

words = dict(ChainMap(*pos_df.select(F.create_map('review_full', 'sent')).rdd.map(lambda x: x[0]).collect()))

plt.imshow(wordcloud.generate_from_frequencies(words))

word_count = (
    pos_df.withColumn('review_full', F.explode(F.split(F.col('review_full'), '\s+')))
    .withColumn('review_full', F.regexp_replace('review_full', '[^\w]', ''))
    .groupBy('review_full')
    .count()
    .sort('count', ascending = False)
)

word_count.show(10)

"""Many lines skipped to separate this from the stuff above.

Textblob testing

Textblob is a Python library for NLP. It includes a simple API for sentiment analysis. We can use this to test the accuracy of our model.
"""

from textblob import TextBlob

def sentiment_score(review):
  return TextBlob(review).sentiment.polarity

sentiment_udf = udf(lambda x: sentiment_score(x), DoubleType())

tbdf = df.select('restaurant_name', 'rating_review', 'review_full',
               sentiment_udf('review_full').alias('sentiment_score'))

tbdf.groupBy('restaurant_name')\
     .agg(avg('sentiment_score').alias('avg_sentiment_score'))\
     .orderBy('avg_sentiment_score', ascending = False) \
     .show()

#Fetch the resturent location and collect sentiment score depending lo location
#Classify resturent depending on food items
#Compare your result with the rating of the resturant